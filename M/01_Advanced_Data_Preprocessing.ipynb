{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdvanced Data Preprocessing and Cleaning Pipeline\\nComprehensive preprocessing for optimal sepsis prediction model performance\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Advanced Data Preprocessing and Cleaning Pipeline\n",
    "Comprehensive preprocessing for optimal sepsis prediction model performance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDataPreprocessor:\n",
    "    \"\"\"Advanced data preprocessing pipeline for sepsis prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.scalers = {}\n",
    "        self.imputers = {}\n",
    "        self.transformers = {}\n",
    "        self.outlier_detectors = {}\n",
    "        self.feature_stats = {}\n",
    "        \n",
    "    def load_and_analyze_data(self, file_path='../Dataset.csv', sample_size=None):\n",
    "        \"\"\"Load and perform initial data analysis\"\"\"\n",
    "        print(\"=== LOADING AND ANALYZING DATASET ===\\n\")\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Original dataset shape: {df.shape}\")\n",
    "        print(f\"Total records: {len(df):,}\")\n",
    "        print(f\"Unique patients: {df['Patient_ID'].nunique():,}\")\n",
    "        \n",
    "        # Sample if needed\n",
    "        if sample_size and len(df) > sample_size:\n",
    "            print(f\"Sampling {sample_size:,} records...\")\n",
    "            # Stratified sampling maintaining class distribution\n",
    "            sepsis_rate = df['SepsisLabel'].mean()\n",
    "            df_majority = df[df['SepsisLabel'] == 0].sample(\n",
    "                n=int(sample_size * (1 - sepsis_rate)), \n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            df_minority = df[df['SepsisLabel'] == 1].sample(\n",
    "                n=int(sample_size * sepsis_rate), \n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            df = pd.concat([df_majority, df_minority]).sample(\n",
    "                frac=1, random_state=self.random_state\n",
    "            ).reset_index(drop=True)\n",
    "            print(f\"Sampled dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"\\n=== DATASET OVERVIEW ===\")\n",
    "        print(f\"Sepsis rate: {df['SepsisLabel'].mean():.3f} ({df['SepsisLabel'].mean()*100:.1f}%)\")\n",
    "        print(f\"Average age: {df['Age'].mean():.1f} years\")\n",
    "        print(f\"Gender distribution: {df['Gender'].value_counts().to_dict()}\")\n",
    "        print(f\"Average ICU stay: {df['ICULOS'].mean():.1f} hours\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def analyze_missing_patterns(self, df):\n",
    "        \"\"\"Comprehensive missing data analysis\"\"\"\n",
    "        print(\"\\n=== MISSING DATA ANALYSIS ===\\n\")\n",
    "        \n",
    "        # Missing value statistics\n",
    "        missing_stats = pd.DataFrame({\n",
    "            'Missing_Count': df.isnull().sum(),\n",
    "            'Missing_Percent': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "            'Data_Type': df.dtypes\n",
    "        }).sort_values('Missing_Percent', ascending=False)\n",
    "        \n",
    "        print(\"Missing value summary:\")\n",
    "        print(missing_stats.head(15))\n",
    "        \n",
    "        # Missing patterns by patient\n",
    "        patient_missing = df.groupby('Patient_ID').apply(\n",
    "            lambda x: x.isnull().sum().sum()\n",
    "        ).describe()\n",
    "        print(f\"\\nMissing values per patient statistics:\")\n",
    "        print(patient_missing)\n",
    "        \n",
    "        # Correlation between missing values and target\n",
    "        missing_indicators = df.isnull().astype(int)\n",
    "        missing_target_corr = missing_indicators.corrwith(df['SepsisLabel']).abs().sort_values(ascending=False)\n",
    "        print(f\"\\nTop 10 missing patterns correlated with sepsis:\")\n",
    "        print(missing_target_corr.head(10))\n",
    "        \n",
    "        self.feature_stats['missing_stats'] = missing_stats\n",
    "        return missing_stats\n",
    "    \n",
    "    def detect_and_handle_outliers(self, df, features):\n",
    "        \"\"\"Advanced outlier detection and handling\"\"\"\n",
    "        print(\"\\n=== OUTLIER DETECTION AND HANDLING ===\\n\")\n",
    "        \n",
    "        outlier_summary = {}\n",
    "        \n",
    "        for feature in features:\n",
    "            if feature in df.columns and df[feature].dtype in ['int64', 'float64']:\n",
    "                # Statistical outlier detection\n",
    "                Q1 = df[feature].quantile(0.25)\n",
    "                Q3 = df[feature].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Z-score outliers\n",
    "                z_scores = np.abs(zscore(df[feature].dropna()))\n",
    "                z_outliers = z_scores > 3\n",
    "                \n",
    "                # Clinical bounds (domain knowledge)\n",
    "                clinical_bounds = self.get_clinical_bounds(feature)\n",
    "                clinical_outliers = (\n",
    "                    (df[feature] < clinical_bounds['min']) | \n",
    "                    (df[feature] > clinical_bounds['max'])\n",
    "                )\n",
    "                \n",
    "                outlier_count = clinical_outliers.sum()\n",
    "                outlier_percent = (outlier_count / len(df)) * 100\n",
    "                \n",
    "                outlier_summary[feature] = {\n",
    "                    'outlier_count': outlier_count,\n",
    "                    'outlier_percent': outlier_percent,\n",
    "                    'clinical_bounds': clinical_bounds\n",
    "                }\n",
    "                \n",
    "                # Handle outliers based on clinical knowledge\n",
    "                if outlier_percent > 0.1:  # If >0.1% outliers\n",
    "                    print(f\"{feature}: {outlier_count} outliers ({outlier_percent:.2f}%)\")\n",
    "                    # Cap outliers to clinical bounds\n",
    "                    df[feature] = df[feature].clip(\n",
    "                        lower=clinical_bounds['min'], \n",
    "                        upper=clinical_bounds['max']\n",
    "                    )\n",
    "        \n",
    "        self.feature_stats['outlier_summary'] = outlier_summary\n",
    "        return df\n",
    "    \n",
    "    def get_clinical_bounds(self, feature):\n",
    "        \"\"\"Get clinically reasonable bounds for features\"\"\"\n",
    "        clinical_bounds = {\n",
    "            'HR': {'min': 20, 'max': 250},\n",
    "            'O2Sat': {'min': 50, 'max': 100},\n",
    "            'Temp': {'min': 30, 'max': 45},\n",
    "            'SBP': {'min': 50, 'max': 300},\n",
    "            'MAP': {'min': 30, 'max': 200},\n",
    "            'DBP': {'min': 20, 'max': 150},\n",
    "            'Resp': {'min': 5, 'max': 60},\n",
    "            'Age': {'min': 0, 'max': 120},\n",
    "            'ICULOS': {'min': 0, 'max': 2000},\n",
    "            'Glucose': {'min': 20, 'max': 1000},\n",
    "            'BUN': {'min': 1, 'max': 200},\n",
    "            'Creatinine': {'min': 0.1, 'max': 20},\n",
    "            'WBC': {'min': 0.1, 'max': 100},\n",
    "            'Hct': {'min': 10, 'max': 70},\n",
    "            'Hgb': {'min': 3, 'max': 25},\n",
    "            'Platelets': {'min': 10, 'max': 2000},\n",
    "            'Lactate': {'min': 0.1, 'max': 30}\n",
    "        }\n",
    "        return clinical_bounds.get(feature, {'min': -np.inf, 'max': np.inf})\n",
    "    \n",
    "    def advanced_feature_engineering(self, df):\n",
    "        \"\"\"Comprehensive clinical feature engineering\"\"\"\n",
    "        print(\"\\n=== ADVANCED FEATURE ENGINEERING ===\\n\")\n",
    "        \n",
    "        original_features = df.shape[1]\n",
    "        \n",
    "        # 1. Clinical Composite Scores\n",
    "        print(\"Creating clinical composite scores...\")\n",
    "        \n",
    "        # SIRS Criteria (Systemic Inflammatory Response Syndrome)\n",
    "        sirs_criteria = 0\n",
    "        if 'Temp' in df.columns:\n",
    "            sirs_criteria += ((df['Temp'] > 38) | (df['Temp'] < 36)).astype(int)\n",
    "        if 'HR' in df.columns:\n",
    "            sirs_criteria += (df['HR'] > 90).astype(int)\n",
    "        if 'Resp' in df.columns:\n",
    "            sirs_criteria += (df['Resp'] > 20).astype(int)\n",
    "        if 'WBC' in df.columns:\n",
    "            sirs_criteria += ((df['WBC'] > 12) | (df['WBC'] < 4)).astype(int)\n",
    "        df['SIRS_Score'] = sirs_criteria\n",
    "        \n",
    "        # qSOFA Score (quick Sequential Organ Failure Assessment)\n",
    "        qsofa_score = 0\n",
    "        if 'SBP' in df.columns:\n",
    "            qsofa_score += (df['SBP'] <= 100).astype(int)\n",
    "        if 'Resp' in df.columns:\n",
    "            qsofa_score += (df['Resp'] >= 22).astype(int)\n",
    "        # GCS would be ideal but not available, use altered mental status proxy\n",
    "        df['qSOFA_Score'] = qsofa_score\n",
    "        \n",
    "        # 2. Vital Sign Ratios and Indices\n",
    "        print(\"Creating vital sign ratios...\")\n",
    "        \n",
    "        if 'HR' in df.columns and 'SBP' in df.columns:\n",
    "            df['Shock_Index'] = df['HR'] / (df['SBP'] + 1e-6)\n",
    "            df['Shock_Index_High'] = (df['Shock_Index'] > 0.9).astype(int)\n",
    "        \n",
    "        if 'SBP' in df.columns and 'DBP' in df.columns:\n",
    "            df['Pulse_Pressure'] = df['SBP'] - df['DBP']\n",
    "            df['Pulse_Pressure_Narrow'] = (df['Pulse_Pressure'] < 25).astype(int)\n",
    "        \n",
    "        if 'MAP' in df.columns:\n",
    "            df['MAP_Critical'] = (df['MAP'] < 65).astype(int)\n",
    "        \n",
    "        # 3. Laboratory Ratios\n",
    "        print(\"Creating laboratory ratios...\")\n",
    "        \n",
    "        if 'BUN' in df.columns and 'Creatinine' in df.columns:\n",
    "            df['BUN_Creatinine_Ratio'] = df['BUN'] / (df['Creatinine'] + 1e-6)\n",
    "            df['AKI_Risk'] = (df['BUN_Creatinine_Ratio'] > 20).astype(int)\n",
    "        \n",
    "        if 'WBC' in df.columns and 'Hct' in df.columns:\n",
    "            df['WBC_Hct_Ratio'] = df['WBC'] / (df['Hct'] + 1e-6)\n",
    "        \n",
    "        # 4. Time-based Features\n",
    "        print(\"Creating temporal features...\")\n",
    "        \n",
    "        if 'ICULOS' in df.columns:\n",
    "            df['ICU_Day'] = (df['ICULOS'] / 24).astype(int)\n",
    "            df['ICU_Hour_of_Day'] = df['ICULOS'] % 24\n",
    "            df['ICU_Early'] = (df['ICULOS'] <= 6).astype(int)\n",
    "            df['ICU_Late'] = (df['ICULOS'] > 72).astype(int)\n",
    "        \n",
    "        # 5. Age-based Risk Categories\n",
    "        print(\"Creating age-based features...\")\n",
    "        \n",
    "        if 'Age' in df.columns:\n",
    "            df['Age_Pediatric'] = (df['Age'] < 18).astype(int)\n",
    "            df['Age_Adult'] = ((df['Age'] >= 18) & (df['Age'] < 65)).astype(int)\n",
    "            df['Age_Elderly'] = ((df['Age'] >= 65) & (df['Age'] < 80)).astype(int)\n",
    "            df['Age_Very_Elderly'] = (df['Age'] >= 80).astype(int)\n",
    "            df['Age_Squared'] = df['Age'] ** 2\n",
    "        \n",
    "        # 6. Clinical Severity Indicators\n",
    "        print(\"Creating severity indicators...\")\n",
    "        \n",
    "        # Multi-organ dysfunction indicators\n",
    "        organ_dysfunction = 0\n",
    "        if 'SBP' in df.columns:\n",
    "            organ_dysfunction += (df['SBP'] < 90).astype(int)  # Cardiovascular\n",
    "        if 'O2Sat' in df.columns:\n",
    "            organ_dysfunction += (df['O2Sat'] < 90).astype(int)  # Respiratory\n",
    "        if 'Creatinine' in df.columns:\n",
    "            organ_dysfunction += (df['Creatinine'] > 2.0).astype(int)  # Renal\n",
    "        if 'Platelets' in df.columns:\n",
    "            organ_dysfunction += (df['Platelets'] < 100).astype(int)  # Hematologic\n",
    "        df['Organ_Dysfunction_Count'] = organ_dysfunction\n",
    "        \n",
    "        # 7. Interaction Features\n",
    "        print(\"Creating interaction features...\")\n",
    "        \n",
    "        if 'Age' in df.columns and 'SIRS_Score' in df.columns:\n",
    "            df['Age_SIRS_Interaction'] = df['Age'] * df['SIRS_Score']\n",
    "        \n",
    "        if 'Gender' in df.columns and 'Age' in df.columns:\n",
    "            df['Gender_Age_Interaction'] = df['Gender'] * df['Age']\n",
    "        \n",
    "        new_features = df.shape[1] - original_features\n",
    "        print(f\"Created {new_features} new features\")\n",
    "        print(f\"Total features: {df.shape[1]}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def advanced_imputation(self, df, target_col='SepsisLabel'):\n",
    "        \"\"\"Advanced missing value imputation strategies\"\"\"\n",
    "        print(\"\\n=== ADVANCED MISSING VALUE IMPUTATION ===\\n\")\n",
    "        \n",
    "        # Separate features by missing percentage\n",
    "        missing_stats = df.isnull().mean()\n",
    "        \n",
    "        low_missing = missing_stats[missing_stats <= 0.3].index.tolist()\n",
    "        medium_missing = missing_stats[(missing_stats > 0.3) & (missing_stats <= 0.7)].index.tolist()\n",
    "        high_missing = missing_stats[missing_stats > 0.7].index.tolist()\n",
    "        \n",
    "        # Remove non-feature columns\n",
    "        exclude_cols = ['Patient_ID', 'Hour', 'Unit1', 'Unit2', 'HospAdmTime', 'Unnamed: 0', target_col]\n",
    "        low_missing = [col for col in low_missing if col not in exclude_cols]\n",
    "        medium_missing = [col for col in medium_missing if col not in exclude_cols]\n",
    "        high_missing = [col for col in high_missing if col not in exclude_cols]\n",
    "        \n",
    "        print(f\"Low missing (<30%): {len(low_missing)} features\")\n",
    "        print(f\"Medium missing (30-70%): {len(medium_missing)} features\")\n",
    "        print(f\"High missing (>70%): {len(high_missing)} features\")\n",
    "        \n",
    "        # Strategy 1: KNN Imputation for low missing features\n",
    "        if low_missing:\n",
    "            print(\"Applying KNN imputation for low missing features...\")\n",
    "            knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "            df[low_missing] = knn_imputer.fit_transform(df[low_missing])\n",
    "            self.imputers['knn'] = knn_imputer\n",
    "        \n",
    "        # Strategy 2: Iterative imputation for medium missing features\n",
    "        if medium_missing:\n",
    "            print(\"Applying iterative imputation for medium missing features...\")\n",
    "            iterative_imputer = IterativeImputer(\n",
    "                estimator=None,  # Uses BayesianRidge by default\n",
    "                max_iter=10,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            df[medium_missing] = iterative_imputer.fit_transform(df[medium_missing])\n",
    "            self.imputers['iterative'] = iterative_imputer\n",
    "        \n",
    "        # Strategy 3: Clinical-informed imputation for high missing features\n",
    "        if high_missing:\n",
    "            print(\"Applying clinical-informed imputation for high missing features...\")\n",
    "            for col in high_missing:\n",
    "                if col in df.columns:\n",
    "                    # Create missing indicator\n",
    "                    df[f'{col}_Missing'] = df[col].isnull().astype(int)\n",
    "                    \n",
    "                    # Impute with clinical normal values\n",
    "                    clinical_normals = {\n",
    "                        'Lactate': 1.5, 'AST': 25, 'BUN': 15, 'Alkalinephos': 100,\n",
    "                        'Calcium': 9.5, 'Chloride': 100, 'Creatinine': 1.0,\n",
    "                        'Bilirubin_direct': 0.2, 'Glucose': 100, 'Magnesium': 2.0,\n",
    "                        'Phosphate': 3.5, 'Potassium': 4.0, 'Bilirubin_total': 1.0,\n",
    "                        'TroponinI': 0.01, 'PTT': 30, 'Fibrinogen': 300,\n",
    "                        'BaseExcess': 0, 'HCO3': 24, 'PaCO2': 40, 'pH': 7.4,\n",
    "                        'SaO2': 98, 'EtCO2': 35, 'FiO2': 21\n",
    "                    }\n",
    "                    \n",
    "                    normal_value = clinical_normals.get(col, df[col].median())\n",
    "                    df[col].fillna(normal_value, inplace=True)\n",
    "        \n",
    "        print(\"Imputation completed\")\n",
    "        print(f\"Remaining missing values: {df.isnull().sum().sum()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def feature_transformation(self, df, target_col='SepsisLabel'):\n",
    "        \"\"\"Advanced feature transformations\"\"\"\n",
    "        print(\"\\n=== FEATURE TRANSFORMATIONS ===\\n\")\n",
    "        \n",
    "        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numeric_features = [col for col in numeric_features if col != target_col]\n",
    "        \n",
    "        # 1. Skewness correction\n",
    "        print(\"Analyzing and correcting skewness...\")\n",
    "        skewed_features = []\n",
    "        \n",
    "        for feature in numeric_features:\n",
    "            if feature in df.columns:\n",
    "                skewness = df[feature].skew()\n",
    "                if abs(skewness) > 1:  # Highly skewed\n",
    "                    skewed_features.append(feature)\n",
    "                    print(f\"{feature}: skewness = {skewness:.2f}\")\n",
    "        \n",
    "        # Apply power transformation to highly skewed features\n",
    "        if skewed_features:\n",
    "            print(f\"Applying power transformation to {len(skewed_features)} skewed features...\")\n",
    "            power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "            df[skewed_features] = power_transformer.fit_transform(df[skewed_features])\n",
    "            self.transformers['power'] = power_transformer\n",
    "        \n",
    "        # 2. Normalization/Standardization\n",
    "        print(\"Applying robust scaling...\")\n",
    "        scaler = RobustScaler()\n",
    "        df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "        self.scalers['robust'] = scaler\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def intelligent_feature_selection(self, df, target_col='SepsisLabel'):\n",
    "        \"\"\"Intelligent feature selection based on clinical relevance and statistical significance\"\"\"\n",
    "        print(\"\\n=== INTELLIGENT FEATURE SELECTION ===\\n\")\n",
    "        \n",
    "        X = df.drop(columns=[target_col, 'Patient_ID'], errors='ignore')\n",
    "        y = df[target_col]\n",
    "        \n",
    "        original_features = X.shape[1]\n",
    "        print(f\"Starting with {original_features} features\")\n",
    "        \n",
    "        # 1. Remove constant and quasi-constant features\n",
    "        print(\"Removing constant and quasi-constant features...\")\n",
    "        variance_selector = VarianceThreshold(threshold=0.01)\n",
    "        X_variance = variance_selector.fit_transform(X)\n",
    "        selected_features = X.columns[variance_selector.get_support()].tolist()\n",
    "        X = X[selected_features]\n",
    "        print(f\"After variance threshold: {X.shape[1]} features\")\n",
    "        \n",
    "        # 2. Statistical significance test\n",
    "        print(\"Selecting statistically significant features...\")\n",
    "        # Use all features if reasonable number, otherwise select top 50\n",
    "        k = min(50, X.shape[1])\n",
    "        selector = SelectKBest(score_func=f_classif, k=k)\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "        selected_feature_names = X.columns[selector.get_support()].tolist()\n",
    "        \n",
    "        # Get feature scores\n",
    "        feature_scores = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'score': selector.scores_,\n",
    "            'p_value': selector.pvalues_\n",
    "        }).sort_values('score', ascending=False)\n",
    "        \n",
    "        print(f\"After statistical selection: {len(selected_feature_names)} features\")\n",
    "        print(\"\\nTop 15 most significant features:\")\n",
    "        print(feature_scores.head(15))\n",
    "        \n",
    "        # 3. Clinical relevance filtering\n",
    "        print(\"Applying clinical relevance filtering...\")\n",
    "        clinical_priority_features = [\n",
    "            'SIRS_Score', 'qSOFA_Score', 'Shock_Index', 'MAP_Critical',\n",
    "            'Organ_Dysfunction_Count', 'Age_Elderly', 'ICU_Late',\n",
    "            'HR', 'Temp', 'SBP', 'Resp', 'Age', 'ICULOS'\n",
    "        ]\n",
    "        \n",
    "        # Ensure clinical priority features are included if available\n",
    "        final_features = selected_feature_names.copy()\n",
    "        for feature in clinical_priority_features:\n",
    "            if feature in X.columns and feature not in final_features:\n",
    "                final_features.append(feature)\n",
    "        \n",
    "        print(f\"Final feature count: {len(final_features)}\")\n",
    "        \n",
    "        # Create final dataset\n",
    "        final_df = df[final_features + [target_col, 'Patient_ID']].copy()\n",
    "        \n",
    "        self.feature_stats['selected_features'] = final_features\n",
    "        self.feature_stats['feature_scores'] = feature_scores\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def generate_preprocessing_report(self, original_df, processed_df):\n",
    "        \"\"\"Generate comprehensive preprocessing report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPREHENSIVE PREPROCESSING REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š DATASET TRANSFORMATION SUMMARY\")\n",
    "        print(f\"Original shape: {original_df.shape}\")\n",
    "        print(f\"Processed shape: {processed_df.shape}\")\n",
    "        print(f\"Features added: {processed_df.shape[1] - original_df.shape[1]}\")\n",
    "        \n",
    "        print(f\"\\nðŸ”§ PREPROCESSING STEPS APPLIED\")\n",
    "        print(\"âœ… Advanced outlier detection and clinical bounds enforcement\")\n",
    "        print(\"âœ… Comprehensive feature engineering (clinical scores, ratios, interactions)\")\n",
    "        print(\"âœ… Multi-strategy missing value imputation\")\n",
    "        print(\"âœ… Skewness correction with power transformations\")\n",
    "        print(\"âœ… Robust scaling for outlier resistance\")\n",
    "        print(\"âœ… Intelligent feature selection with clinical prioritization\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ DATA QUALITY IMPROVEMENTS\")\n",
    "        original_missing = original_df.isnull().sum().sum()\n",
    "        processed_missing = processed_df.isnull().sum().sum()\n",
    "        print(f\"Missing values: {original_missing:,} â†’ {processed_missing:,}\")\n",
    "        print(f\"Missing value reduction: {((original_missing - processed_missing) / original_missing * 100):.1f}%\")\n",
    "        \n",
    "        if 'selected_features' in self.feature_stats:\n",
    "            print(f\"\\nðŸŽ¯ FEATURE SELECTION RESULTS\")\n",
    "            print(f\"Selected features: {len(self.feature_stats['selected_features'])}\")\n",
    "            print(\"Top clinical features included:\")\n",
    "            clinical_features = [f for f in self.feature_stats['selected_features'] \n",
    "                               if any(keyword in f.lower() for keyword in \n",
    "                                    ['sirs', 'qsofa', 'shock', 'organ', 'age', 'icu'])]\n",
    "            for feature in clinical_features[:10]:\n",
    "                print(f\"  â€¢ {feature}\")\n",
    "        \n",
    "        return processed_df\n",
    "    \n",
    "    def save_preprocessing_artifacts(self, processed_df, prefix='advanced'):\n",
    "        \"\"\"Save all preprocessing artifacts\"\"\"\n",
    "        print(f\"\\nðŸ’¾ SAVING PREPROCESSING ARTIFACTS\")\n",
    "        \n",
    "        # Save processed dataset\n",
    "        processed_df.to_csv(f'{prefix}_processed_dataset.csv', index=False)\n",
    "        print(f\"âœ… Saved: {prefix}_processed_dataset.csv\")\n",
    "        \n",
    "        # Save preprocessing components\n",
    "        joblib.dump(self.scalers, f'{prefix}_scalers.pkl')\n",
    "        joblib.dump(self.imputers, f'{prefix}_imputers.pkl')\n",
    "        joblib.dump(self.transformers, f'{prefix}_transformers.pkl')\n",
    "        joblib.dump(self.feature_stats, f'{prefix}_feature_stats.pkl')\n",
    "        \n",
    "        print(f\"âœ… Saved: {prefix}_scalers.pkl\")\n",
    "        print(f\"âœ… Saved: {prefix}_imputers.pkl\")\n",
    "        print(f\"âœ… Saved: {prefix}_transformers.pkl\")\n",
    "        print(f\"âœ… Saved: {prefix}_feature_stats.pkl\")\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main preprocessing pipeline\"\"\"\n",
    "    print(\"=== ADVANCED DATA PREPROCESSING PIPELINE ===\\n\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = AdvancedDataPreprocessor(random_state=42)\n",
    "    \n",
    "    # Load and analyze data\n",
    "    df = preprocessor.load_and_analyze_data(sample_size=300000)  # Larger sample\n",
    "    original_df = df.copy()\n",
    "    \n",
    "    # Comprehensive preprocessing pipeline\n",
    "    missing_stats = preprocessor.analyze_missing_patterns(df)\n",
    "    \n",
    "    # Select features with reasonable availability\n",
    "    available_features = missing_stats[missing_stats['Missing_Percent'] < 80].index.tolist()\n",
    "    available_features = [f for f in available_features if f not in \n",
    "                         ['Unnamed: 0', 'Hour', 'Unit1', 'Unit2', 'HospAdmTime', 'Patient_ID', 'SepsisLabel']]\n",
    "    \n",
    "    df = preprocessor.detect_and_handle_outliers(df, available_features)\n",
    "    df = preprocessor.advanced_feature_engineering(df)\n",
    "    df = preprocessor.advanced_imputation(df)\n",
    "    df = preprocessor.feature_transformation(df)\n",
    "    df = preprocessor.intelligent_feature_selection(df)\n",
    "    \n",
    "    # Generate report and save artifacts\n",
    "    processed_df = preprocessor.generate_preprocessing_report(original_df, df)\n",
    "    preprocessor.save_preprocessing_artifacts(processed_df)\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ PREPROCESSING COMPLETE!\")\n",
    "    print(\"Ready for advanced model training with cleaned and optimized dataset.\")\n",
    "    \n",
    "    return processed_df, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED DATA PREPROCESSING PIPELINE ===\n",
      "\n",
      "=== LOADING AND ANALYZING DATASET ===\n",
      "\n",
      "Original dataset shape: (1552210, 44)\n",
      "Total records: 1,552,210\n",
      "Unique patients: 40,336\n",
      "Sampling 300,000 records...\n",
      "Sampled dataset shape: (299999, 44)\n",
      "\n",
      "=== DATASET OVERVIEW ===\n",
      "Sepsis rate: 0.018 (1.8%)\n",
      "Average age: 62.0 years\n",
      "Gender distribution: {1: 167445, 0: 132554}\n",
      "Average ICU stay: 27.0 hours\n",
      "\n",
      "=== MISSING DATA ANALYSIS ===\n",
      "\n",
      "Missing value summary:\n",
      "                  Missing_Count  Missing_Percent Data_Type\n",
      "Bilirubin_direct         299375            99.79   float64\n",
      "Fibrinogen               298014            99.34   float64\n",
      "TroponinI                297160            99.05   float64\n",
      "Bilirubin_total          295533            98.51   float64\n",
      "Alkalinephos             295161            98.39   float64\n",
      "AST                      295101            98.37   float64\n",
      "Lactate                  292042            97.35   float64\n",
      "PTT                      291205            97.07   float64\n",
      "SaO2                     289842            96.61   float64\n",
      "EtCO2                    288767            96.26   float64\n",
      "Phosphate                288012            96.00   float64\n",
      "HCO3                     287628            95.88   float64\n",
      "Chloride                 286552            95.52   float64\n",
      "BaseExcess               283662            94.55   float64\n",
      "PaCO2                    283292            94.43   float64\n",
      "\n",
      "Missing values per patient statistics:\n",
      "count    39986.000000\n",
      "mean       210.363877\n",
      "std        140.552056\n",
      "min          9.000000\n",
      "25%        122.000000\n",
      "50%        192.500000\n",
      "75%        269.000000\n",
      "max       2211.000000\n",
      "dtype: float64\n",
      "\n",
      "Top 10 missing patterns correlated with sepsis:\n",
      "EtCO2           0.050475\n",
      "FiO2            0.047832\n",
      "Lactate         0.036669\n",
      "pH              0.033935\n",
      "PaCO2           0.032765\n",
      "BaseExcess      0.028100\n",
      "SaO2            0.021960\n",
      "Potassium       0.016426\n",
      "Alkalinephos    0.015731\n",
      "Unit2           0.015272\n",
      "dtype: float64\n",
      "\n",
      "=== OUTLIER DETECTION AND HANDLING ===\n",
      "\n",
      "Resp: 506 outliers (0.17%)\n",
      "\n",
      "=== ADVANCED FEATURE ENGINEERING ===\n",
      "\n",
      "Creating clinical composite scores...\n",
      "Creating vital sign ratios...\n",
      "Creating laboratory ratios...\n",
      "Creating temporal features...\n",
      "Creating age-based features...\n",
      "Creating severity indicators...\n",
      "Creating interaction features...\n",
      "Created 22 new features\n",
      "Total features: 66\n",
      "\n",
      "=== ADVANCED MISSING VALUE IMPUTATION ===\n",
      "\n",
      "Low missing (<30%): 27 features\n",
      "Medium missing (30-70%): 3 features\n",
      "High missing (>70%): 29 features\n",
      "Applying KNN imputation for low missing features...\n",
      "Applying iterative imputation for medium missing features...\n",
      "Applying clinical-informed imputation for high missing features...\n",
      "Imputation completed\n",
      "Remaining missing values: 237146\n",
      "\n",
      "=== FEATURE TRANSFORMATIONS ===\n",
      "\n",
      "Analyzing and correcting skewness...\n",
      "Unnamed: 0: skewness = 4.08\n",
      "Hour: skewness = 4.08\n",
      "O2Sat: skewness = -4.22\n",
      "MAP: skewness = 1.01\n",
      "DBP: skewness = 1.25\n",
      "Resp: skewness = 1.06\n",
      "BaseExcess: skewness = -2.38\n",
      "FiO2: skewness = -3.01\n",
      "pH: skewness = -5.75\n",
      "PaCO2: skewness = 7.82\n",
      "SaO2: skewness = -15.70\n",
      "AST: skewness = 47.98\n",
      "BUN: skewness = 11.07\n",
      "Alkalinephos: skewness = 96.06\n",
      "Calcium: skewness = -8.90\n",
      "Chloride: skewness = 6.21\n",
      "Creatinine: skewness = 19.88\n",
      "Bilirubin_direct: skewness = 86.60\n",
      "Glucose: skewness = 6.82\n",
      "Lactate: skewness = 21.87\n",
      "Magnesium: skewness = 9.33\n",
      "Phosphate: skewness = 8.57\n",
      "Potassium: skewness = 5.99\n",
      "Bilirubin_total: skewness = 46.67\n",
      "TroponinI: skewness = 83.82\n",
      "Hct: skewness = 2.54\n",
      "Hgb: skewness = 2.21\n",
      "PTT: skewness = 21.26\n",
      "WBC: skewness = 53.44\n",
      "Fibrinogen: skewness = 16.09\n",
      "Platelets: skewness = 8.22\n",
      "HospAdmTime: skewness = -11.89\n",
      "ICULOS: skewness = 4.10\n",
      "qSOFA_Score: skewness = 1.24\n",
      "Shock_Index: skewness = 1.12\n",
      "Shock_Index_High: skewness = 2.13\n",
      "Pulse_Pressure_Narrow: skewness = 7.97\n",
      "MAP_Critical: skewness = 2.69\n",
      "BUN_Creatinine_Ratio: skewness = 11.68\n",
      "AKI_Risk: skewness = 6.64\n",
      "WBC_Hct_Ratio: skewness = 61.90\n",
      "ICU_Day: skewness = 4.13\n",
      "ICU_Early: skewness = 2.03\n",
      "ICU_Late: skewness = 4.49\n",
      "Age_Pediatric: skewness = 46.26\n",
      "Age_Very_Elderly: skewness = 2.14\n",
      "Organ_Dysfunction_Count: skewness = 4.31\n",
      "Age_SIRS_Interaction: skewness = 1.18\n",
      "EtCO2_Missing: skewness = -4.87\n",
      "BaseExcess_Missing: skewness = -3.93\n",
      "HCO3_Missing: skewness = -4.61\n",
      "FiO2_Missing: skewness = -3.01\n",
      "pH_Missing: skewness = -3.40\n",
      "PaCO2_Missing: skewness = -3.87\n",
      "SaO2_Missing: skewness = -5.15\n",
      "AST_Missing: skewness = -7.63\n",
      "BUN_Missing: skewness = -3.42\n",
      "Alkalinephos_Missing: skewness = -7.68\n",
      "Calcium_Missing: skewness = -3.76\n",
      "Chloride_Missing: skewness = -4.40\n",
      "Creatinine_Missing: skewness = -3.69\n",
      "Bilirubin_direct_Missing: skewness = -21.86\n",
      "Glucose_Missing: skewness = -1.75\n",
      "Lactate_Missing: skewness = -5.89\n",
      "Magnesium_Missing: skewness = -3.61\n",
      "Phosphate_Missing: skewness = -4.70\n",
      "Potassium_Missing: skewness = -2.82\n",
      "Bilirubin_total_Missing: skewness = -8.01\n",
      "TroponinI_Missing: skewness = -10.13\n",
      "Hct_Missing: skewness = -2.90\n",
      "Hgb_Missing: skewness = -3.25\n",
      "PTT_Missing: skewness = -5.58\n",
      "WBC_Missing: skewness = -3.56\n",
      "Fibrinogen_Missing: skewness = -12.17\n",
      "Platelets_Missing: skewness = -3.74\n",
      "BUN_Creatinine_Ratio_Missing: skewness = -3.69\n",
      "WBC_Hct_Ratio_Missing: skewness = -3.56\n",
      "Applying power transformation to 77 skewed features...\n",
      "Applying robust scaling...\n",
      "\n",
      "=== INTELLIGENT FEATURE SELECTION ===\n",
      "\n",
      "Starting with 93 features\n",
      "Removing constant and quasi-constant features...\n",
      "After variance threshold: 67 features\n",
      "Selecting statistically significant features...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     processed_df, preprocessor = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m df = preprocessor.advanced_imputation(df)\n\u001b[32m     23\u001b[39m df = preprocessor.feature_transformation(df)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m df = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintelligent_feature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Generate report and save artifacts\u001b[39;00m\n\u001b[32m     27\u001b[39m processed_df = preprocessor.generate_preprocessing_report(original_df, df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 374\u001b[39m, in \u001b[36mAdvancedDataPreprocessor.intelligent_feature_selection\u001b[39m\u001b[34m(self, df, target_col)\u001b[39m\n\u001b[32m    372\u001b[39m k = \u001b[38;5;28mmin\u001b[39m(\u001b[32m50\u001b[39m, X.shape[\u001b[32m1\u001b[39m])\n\u001b[32m    373\u001b[39m selector = SelectKBest(score_func=f_classif, k=k)\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m X_selected = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m selected_feature_names = X.columns[selector.get_support()].tolist()\n\u001b[32m    377\u001b[39m \u001b[38;5;66;03m# Get feature scores\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:921\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:564\u001b[39m, in \u001b[36m_BaseFilter.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    562\u001b[39m     X = validate_data(\u001b[38;5;28mself\u001b[39m, X, accept_sparse=[\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params(X, y)\n\u001b[32m    569\u001b[39m score_func_ret = \u001b[38;5;28mself\u001b[39m.score_func(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    processed_df, preprocessor = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
