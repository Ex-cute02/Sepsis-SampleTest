{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3\n", "\"\"\"\n", "Ultimate Model Training with Advanced Preprocessing\n", "Training sepsis prediction model on comprehensively cleaned and preprocessed data\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n", "                           roc_curve, precision_recall_curve, average_precision_score,\n", "                           accuracy_score, precision_score, recall_score, f1_score)\n", "import xgboost as xgb\n", "import lightgbm as lgb\n", "from imblearn.ensemble import BalancedRandomForestClassifier\n", "from imblearn.over_sampling import SMOTE, ADASYN\n", "from imblearn.under_sampling import EditedNearestNeighbours\n", "from imblearn.combine import SMOTEENN\n", "import joblib\n", "import matplotlib.pyplot as plt\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class UltimateModelTrainer:\n", "    \"\"\"Ultimate model training with ensemble methods and advanced techniques\"\"\"\n", "    \n", "    def __init__(self, random_state=42):\n", "        self.random_state = random_state\n", "        self.models = {}\n", "        self.best_model = None\n", "        self.results = {}\n", "        \n", "    def load_preprocessed_data(self, file_path='advanced_processed_dataset.csv'):\n", "        \"\"\"Load the preprocessed dataset\"\"\"\n", "        print(\"=== LOADING PREPROCESSED DATASET ===\\n\")\n", "        \n", "        try:\n", "            df = pd.read_csv(file_path)\n", "            print(f\"Loaded preprocessed dataset: {df.shape}\")\n", "            print(f\"Features: {df.shape[1] - 2}\")  # Excluding target and Patient_ID\n", "            print(f\"Sepsis rate: {df['SepsisLabel'].mean():.3f}\")\n", "            return df\n", "        except FileNotFoundError:\n", "            print(\"Preprocessed dataset not found. Please run Advanced_Data_Preprocessing.py first.\")\n", "            return None\n", "    \n", "    def prepare_data(self, df):\n", "        \"\"\"Prepare data for training\"\"\"\n", "        print(\"\\n=== PREPARING DATA FOR TRAINING ===\\n\")\n", "        \n", "        # Separate features and target\n", "        X = df.drop(columns=['SepsisLabel', 'Patient_ID'], errors='ignore')\n", "        y = df['SepsisLabel']\n", "        \n", "        print(f\"Feature matrix: {X.shape}\")\n", "        print(f\"Target distribution: {y.value_counts().to_dict()}\")\n", "        \n", "        # Patient-level split to prevent data leakage\n", "        if 'Patient_ID' in df.columns:\n", "            print(\"Performing patient-level split to prevent data leakage...\")\n", "            unique_patients = df['Patient_ID'].unique()\n", "            \n", "            # Stratify by patient-level sepsis occurrence\n", "            patient_labels = df.groupby('Patient_ID')['SepsisLabel'].max()\n", "            \n", "            train_patients, test_patients = train_test_split(\n", "                unique_patients, \n", "                test_size=0.2, \n", "                random_state=self.random_state,\n", "                stratify=patient_labels\n", "            )\n", "            \n", "            train_mask = df['Patient_ID'].isin(train_patients)\n", "            test_mask = df['Patient_ID'].isin(test_patients)\n", "            \n", "            X_train, X_test = X[train_mask], X[test_mask]\n", "            y_train, y_test = y[train_mask], y[test_mask]\n", "            \n", "            print(f\"Patient-level split:\")\n", "            print(f\"  Train patients: {len(train_patients):,}\")\n", "            print(f\"  Test patients: {len(test_patients):,}\")\n", "            print(f\"  Train records: {len(X_train):,}\")\n", "            print(f\"  Test records: {len(X_test):,}\")\n", "        else:\n", "            # Standard split if no Patient_ID\n", "            X_train, X_test, y_train, y_test = train_test_split(\n", "                X, y, test_size=0.2, random_state=self.random_state, stratify=y\n", "            )\n", "        \n", "        print(f\"Training target distribution: {y_train.value_counts().to_dict()}\")\n", "        print(f\"Test target distribution: {y_test.value_counts().to_dict()}\")\n", "        \n", "        return X_train, X_test, y_train, y_test\n", "    \n", "    def optimize_class_balance(self, X_train, y_train):\n", "        \"\"\"Find optimal class balancing strategy\"\"\"\n", "        print(\"\\n=== OPTIMIZING CLASS BALANCE ===\\n\")\n", "        \n", "        # Test multiple sampling strategies\n", "        sampling_strategies = {\n", "            'None': None,\n", "            'SMOTE_Conservative': SMOTE(sampling_strategy=0.1, random_state=self.random_state),\n", "            'ADASYN_Conservative': ADASYN(sampling_strategy=0.15, random_state=self.random_state),\n", "            'SMOTEENN': SMOTEENN(sampling_strategy=0.2, random_state=self.random_state),\n", "        }\n", "        \n", "        best_strategy = None\n", "        best_score = 0\n", "        \n", "        for strategy_name, sampler in sampling_strategies.items():\n", "            try:\n", "                print(f\"Testing {strategy_name}...\")\n", "                \n", "                if sampler is None:\n", "                    X_resampled, y_resampled = X_train, y_train\n", "                else:\n", "                    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n", "                \n", "                print(f\"  Resampled distribution: {pd.Series(y_resampled).value_counts().to_dict()}\")\n", "                \n", "                # Quick validation with balanced random forest\n", "                brf = BalancedRandomForestClassifier(\n", "                    n_estimators=50, \n", "                    random_state=self.random_state,\n", "                    n_jobs=-1\n", "                )\n", "                \n", "                cv_scores = cross_val_score(\n", "                    brf, X_resampled, y_resampled, \n", "                    cv=3, scoring='roc_auc', n_jobs=-1\n", "                )\n", "                \n", "                avg_score = cv_scores.mean()\n", "                std_score = cv_scores.std()\n", "                print(f\"  CV AUC: {avg_score:.3f} \u00c2\u00b1 {std_score:.3f}\")\n", "                \n", "                if avg_score > best_score:\n", "                    best_score = avg_score\n", "                    best_strategy = (strategy_name, sampler)\n", "                    \n", "            except Exception as e:\n", "                print(f\"  Error with {strategy_name}: {e}\")\n", "        \n", "        if best_strategy:\n", "            print(f\"\\nBest sampling strategy: {best_strategy[0]} (AUC: {best_score:.3f})\")\n", "            if best_strategy[1] is None:\n", "                return X_train, y_train\n", "            else:\n", "                return best_strategy[1].fit_resample(X_train, y_train)\n", "        else:\n", "            return X_train, y_train\n", "    \n", "    def train_ensemble_models(self, X_train, y_train, X_test, y_test):\n", "        \"\"\"Train multiple models and create ensemble\"\"\"\n", "        print(\"\\n=== TRAINING ENSEMBLE MODELS ===\\n\")\n", "        \n", "        # Define base models with optimized parameters\n", "        base_models = {\n", "            'XGBoost': xgb.XGBClassifier(\n", "                objective='binary:logistic',\n", "                eval_metric='auc',\n", "                max_depth=4,\n", "                learning_rate=0.05,\n", "                n_estimators=300,\n", "                subsample=0.8,\n", "                colsample_bytree=0.8,\n", "                reg_alpha=1,\n", "                reg_lambda=1,\n", "                min_child_weight=5,\n", "                random_state=self.random_state,\n", "                n_jobs=-1\n", "            ),\n", "            \n", "            'LightGBM': lgb.LGBMClassifier(\n", "                objective='binary',\n", "                metric='auc',\n", "                max_depth=4,\n", "                learning_rate=0.05,\n", "                n_estimators=300,\n", "                subsample=0.8,\n", "                colsample_bytree=0.8,\n", "                reg_alpha=1,\n", "                reg_lambda=1,\n", "                min_child_samples=20,\n", "                random_state=self.random_state,\n", "                n_jobs=-1,\n", "                verbose=-1\n", "            ),\n", "            \n", "            'BalancedRandomForest': BalancedRandomForestClassifier(\n", "                n_estimators=200,\n", "                max_depth=10,\n", "                min_samples_split=10,\n", "                min_samples_leaf=5,\n", "                random_state=self.random_state,\n", "                n_jobs=-1\n", "            ),\n", "            \n", "            'GradientBoosting': GradientBoostingClassifier(\n", "                n_estimators=200,\n", "                learning_rate=0.05,\n", "                max_depth=4,\n", "                subsample=0.8,\n", "                random_state=self.random_state\n", "            )\n", "        }\n", "        \n", "        # Train and evaluate each model\n", "        model_results = {}\n", "        \n", "        for model_name, model in base_models.items():\n", "            print(f\"Training {model_name}...\")\n", "            \n", "            # Train model\n", "            model.fit(X_train, y_train)\n", "            \n", "            # Predictions\n", "            y_pred = model.predict(X_test)\n", "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n", "            \n", "            # Metrics\n", "            results = {\n", "                'accuracy': accuracy_score(y_test, y_pred),\n", "                'precision': precision_score(y_test, y_pred),\n", "                'recall': recall_score(y_test, y_pred),\n", "                'f1_score': f1_score(y_test, y_pred),\n", "                'auc_roc': roc_auc_score(y_test, y_pred_proba),\n", "                'avg_precision': average_precision_score(y_test, y_pred_proba)\n", "            }\n", "            \n", "            model_results[model_name] = results\n", "            self.models[model_name] = model\n", "            \n", "            print(f\"  AUC-ROC: {results['auc_roc']:.3f}\")\n", "            print(f\"  Precision: {results['precision']:.3f}\")\n", "            print(f\"  Recall: {results['recall']:.3f}\")\n", "        \n", "        # Create ensemble model\n", "        print(\"\\nCreating ensemble model...\")\n", "        \n", "        # Select top 3 models based on AUC\n", "        top_models = sorted(model_results.items(), key=lambda x: x[1]['auc_roc'], reverse=True)[:3]\n", "        print(f\"Top models for ensemble: {[name for name, _ in top_models]}\")\n", "        \n", "        ensemble_estimators = [(name, self.models[name]) for name, _ in top_models]\n", "        \n", "        ensemble_model = VotingClassifier(\n", "            estimators=ensemble_estimators,\n", "            voting='soft',\n", "            n_jobs=-1\n", "        )\n", "        \n", "        print(\"Training ensemble model...\")\n", "        ensemble_model.fit(X_train, y_train)\n", "        \n", "        # Evaluate ensemble\n", "        y_pred_ensemble = ensemble_model.predict(X_test)\n", "        y_pred_proba_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n", "        \n", "        ensemble_results = {\n", "            'accuracy': accuracy_score(y_test, y_pred_ensemble),\n", "            'precision': precision_score(y_test, y_pred_ensemble),\n", "            'recall': recall_score(y_test, y_pred_ensemble),\n", "            'f1_score': f1_score(y_test, y_pred_ensemble),\n", "            'auc_roc': roc_auc_score(y_test, y_pred_proba_ensemble),\n", "            'avg_precision': average_precision_score(y_test, y_pred_proba_ensemble)\n", "        }\n", "        \n", "        model_results['Ensemble'] = ensemble_results\n", "        self.models['Ensemble'] = ensemble_model\n", "        \n", "        print(f\"Ensemble AUC-ROC: {ensemble_results['auc_roc']:.3f}\")\n", "        \n", "        self.results = model_results\n", "        return model_results\n", "    \n", "    def hyperparameter_optimization(self, X_train, y_train, model_name='XGBoost'):\n", "        \"\"\"Advanced hyperparameter optimization\"\"\"\n", "        print(f\"\\n=== HYPERPARAMETER OPTIMIZATION FOR {model_name} ===\\n\")\n", "        \n", "        if model_name == 'XGBoost':\n", "            param_grid = {\n", "                'max_depth': [3, 4, 5, 6],\n", "                'learning_rate': [0.01, 0.05, 0.1],\n", "                'n_estimators': [200, 300, 500],\n", "                'subsample': [0.8, 0.9],\n", "                'colsample_bytree': [0.8, 0.9],\n", "                'reg_alpha': [0.1, 1, 10],\n", "                'reg_lambda': [0.1, 1, 10]\n", "            }\n", "            \n", "            base_model = xgb.XGBClassifier(\n", "                objective='binary:logistic',\n", "                eval_metric='auc',\n", "                random_state=self.random_state,\n", "                n_jobs=-1\n", "            )\n", "        \n", "        elif model_name == 'LightGBM':\n", "            param_grid = {\n", "                'max_depth': [3, 4, 5, 6],\n", "                'learning_rate': [0.01, 0.05, 0.1],\n", "                'n_estimators': [200, 300, 500],\n", "                'subsample': [0.8, 0.9],\n", "                'colsample_bytree': [0.8, 0.9],\n", "                'reg_alpha': [0.1, 1, 10],\n", "                'reg_lambda': [0.1, 1, 10]\n", "            }\n", "            \n", "            base_model = lgb.LGBMClassifier(\n", "                objective='binary',\n", "                metric='auc',\n", "                random_state=self.random_state,\n", "                n_jobs=-1,\n", "                verbose=-1\n", "            )\n", "        \n", "        # Randomized search for efficiency\n", "        from sklearn.model_selection import RandomizedSearchCV\n", "        \n", "        random_search = RandomizedSearchCV(\n", "            base_model,\n", "            param_grid,\n", "            n_iter=50,  # Reduced for efficiency\n", "            cv=3,\n", "            scoring='roc_auc',\n", "            n_jobs=-1,\n", "            random_state=self.random_state,\n", "            verbose=1\n", "        )\n", "        \n", "        print(\"Performing randomized hyperparameter search...\")\n", "        random_search.fit(X_train, y_train)\n", "        \n", "        print(f\"Best parameters: {random_search.best_params_}\")\n", "        print(f\"Best CV score: {random_search.best_score_:.3f}\")\n", "        \n", "        return random_search.best_estimator_\n", "    \n", "    def comprehensive_evaluation(self, X_test, y_test):\n", "        \"\"\"Comprehensive model evaluation and comparison\"\"\"\n", "        print(\"\\n=== COMPREHENSIVE MODEL EVALUATION ===\\n\")\n", "        \n", "        # Find best model\n", "        best_model_name = max(self.results.keys(), key=lambda k: self.results[k]['auc_roc'])\n", "        best_model = self.models[best_model_name]\n", "        best_auc = self.results[best_model_name]['auc_roc']\n", "        \n", "        print(f\"Best model: {best_model_name} (AUC: {best_auc:.3f})\")\n", "        \n", "        # Detailed evaluation of best model\n", "        y_pred = best_model.predict(X_test)\n", "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n", "        \n", "        print(f\"\\n=== {best_model_name.upper()} DETAILED RESULTS ===\")\n", "        print(f\"AUC-ROC: {self.results[best_model_name]['auc_roc']:.3f}\")\n", "        print(f\"Accuracy: {self.results[best_model_name]['accuracy']:.3f}\")\n", "        print(f\"Precision: {self.results[best_model_name]['precision']:.3f}\")\n", "        print(f\"Recall: {self.results[best_model_name]['recall']:.3f}\")\n", "        print(f\"F1-Score: {self.results[best_model_name]['f1_score']:.3f}\")\n", "        \n", "        print(\"\\nClassification Report:\")\n", "        print(classification_report(y_test, y_pred, target_names=['No Sepsis', 'Sepsis']))\n", "        \n", "        print(\"\\nConfusion Matrix:\")\n", "        cm = confusion_matrix(y_test, y_pred)\n", "        print(cm)\n", "        \n", "        # Clinical metrics\n", "        tn, fp, fn, tp = cm.ravel()\n", "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n", "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n", "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n", "        \n", "        print(f\"\\nClinical Metrics:\")\n", "        print(f\"Sensitivity (Recall): {self.results[best_model_name]['recall']:.3f}\")\n", "        print(f\"Specificity: {specificity:.3f}\")\n", "        print(f\"PPV (Precision): {ppv:.3f}\")\n", "        print(f\"NPV: {npv:.3f}\")\n", "        \n", "        # Model comparison table\n", "        print(f\"\\n=== MODEL COMPARISON TABLE ===\")\n", "        comparison_df = pd.DataFrame(self.results).T\n", "        comparison_df = comparison_df.round(3)\n", "        print(comparison_df.sort_values('auc_roc', ascending=False))\n", "        \n", "        self.best_model = best_model\n", "        return best_model, best_model_name\n", "    \n", "    def save_ultimate_model(self, best_model, best_model_name, X_train):\n", "        \"\"\"Save the ultimate model and artifacts\"\"\"\n", "        print(f\"\\n=== SAVING ULTIMATE MODEL ===\\n\")\n", "        \n", "        # Save best model\n", "        joblib.dump(best_model, 'ultimate_sepsis_model.pkl')\n", "        print(f\"\u00e2\u0153\u2026 Saved: ultimate_sepsis_model.pkl ({best_model_name})\")\n", "        \n", "        # Save feature names\n", "        feature_names = X_train.columns.tolist()\n", "        joblib.dump(feature_names, 'ultimate_feature_names.pkl')\n", "        print(f\"\u00e2\u0153\u2026 Saved: ultimate_feature_names.pkl\")\n", "        \n", "        # Save all results\n", "        joblib.dump(self.results, 'ultimate_model_results.pkl')\n", "        print(f\"\u00e2\u0153\u2026 Saved: ultimate_model_results.pkl\")\n", "        \n", "        # Save all models for ensemble use\n", "        joblib.dump(self.models, 'ultimate_all_models.pkl')\n", "        print(f\"\u00e2\u0153\u2026 Saved: ultimate_all_models.pkl\")\n", "        \n", "        # Feature importance (if available)\n", "        if hasattr(best_model, 'feature_importances_'):\n", "            importance_df = pd.DataFrame({\n", "                'feature': feature_names,\n", "                'importance': best_model.feature_importances_\n", "            }).sort_values('importance', ascending=False)\n", "            \n", "            importance_df.to_csv('ultimate_feature_importance.csv', index=False)\n", "            print(f\"\u00e2\u0153\u2026 Saved: ultimate_feature_importance.csv\")\n", "            \n", "            print(f\"\\nTop 15 Feature Importance:\")\n", "            print(importance_df.head(15))\n", "        \n", "        return True\n", "    \n", "    def clinical_interpretation(self, best_auc):\n", "        \"\"\"Provide clinical interpretation\"\"\"\n", "        print(f\"\\n=== CLINICAL INTERPRETATION ===\")\n", "        \n", "        if best_auc >= 0.9:\n", "            rating = \"EXCELLENT\"\n", "            description = \"Outstanding discrimination - Ready for clinical deployment\"\n", "        elif best_auc >= 0.8:\n", "            rating = \"VERY GOOD\"\n", "            description = \"Strong predictive performance - Clinical validation recommended\"\n", "        elif best_auc >= 0.75:\n", "            rating = \"GOOD\"\n", "            description = \"Good predictive performance - Further validation needed\"\n", "        elif best_auc >= 0.7:\n", "            rating = \"FAIR\"\n", "            description = \"Acceptable performance - May have limited clinical utility\"\n", "        else:\n", "            rating = \"POOR\"\n", "            description = \"Limited clinical utility - Needs improvement\"\n", "        \n", "        print(f\"AUC Score: {best_auc:.3f}\")\n", "        print(f\"Rating: {rating}\")\n", "        print(f\"Description: {description}\")\n", "        \n", "        return rating"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n", "    \"\"\"Main ultimate training pipeline\"\"\"\n", "    print(\"=== ULTIMATE MODEL TRAINING PIPELINE ===\\n\")\n", "    \n", "    # Initialize trainer\n", "    trainer = UltimateModelTrainer(random_state=42)\n", "    \n", "    # Load preprocessed data\n", "    df = trainer.load_preprocessed_data()\n", "    if df is None:\n", "        print(\"Please run Advanced_Data_Preprocessing.py first to create preprocessed dataset.\")\n", "        return\n", "    \n", "    # Prepare data\n", "    X_train, X_test, y_train, y_test = trainer.prepare_data(df)\n", "    \n", "    # Optimize class balance\n", "    X_train_balanced, y_train_balanced = trainer.optimize_class_balance(X_train, y_train)\n", "    \n", "    # Train ensemble models\n", "    model_results = trainer.train_ensemble_models(X_train_balanced, y_train_balanced, X_test, y_test)\n", "    \n", "    # Comprehensive evaluation\n", "    best_model, best_model_name = trainer.comprehensive_evaluation(X_test, y_test)\n", "    \n", "    # Save ultimate model\n", "    trainer.save_ultimate_model(best_model, best_model_name, X_train)\n", "    \n", "    # Clinical interpretation\n", "    best_auc = model_results[best_model_name]['auc_roc']\n", "    rating = trainer.clinical_interpretation(best_auc)\n", "    \n", "    print(f\"\\n\u00f0\u0178\u017d\u2030 ULTIMATE MODEL TRAINING COMPLETE!\")\n", "    print(f\"Best Model: {best_model_name}\")\n", "    print(f\"Performance: {best_auc:.3f} AUC ({rating})\")\n", "    print(\"Ready for clinical validation and deployment!\")\n", "    \n", "    return trainer, best_model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    trainer, best_model = main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}