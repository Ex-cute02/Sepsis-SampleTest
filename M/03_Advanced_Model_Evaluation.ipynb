{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdvanced Model Evaluation and Comparison\\nComprehensive evaluation of the optimized sepsis prediction model\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Advanced Model Evaluation and Comparison\n",
    "Comprehensive evaluation of the optimized sepsis prediction model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           roc_curve, precision_recall_curve, average_precision_score,\n",
    "                           accuracy_score, precision_score, recall_score, f1_score)\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred_proba, title=\"ROC Curve\"):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(y_true, y_pred_proba, title=\"Precision-Recall Curve\"):\n",
    "    \"\"\"Plot Precision-Recall curve\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {avg_precision:.3f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_comprehensive(model, X_test, y_test, feature_names):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    print(\"=== COMPREHENSIVE MODEL EVALUATION ===\\n\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(\"=== PERFORMANCE METRICS ===\")\n",
    "    print(f\"Accuracy:           {accuracy:.4f}\")\n",
    "    print(f\"Precision:          {precision:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "    print(f\"F1-Score:           {f1:.4f}\")\n",
    "    print(f\"AUC-ROC:            {auc:.4f}\")\n",
    "    print(f\"Average Precision:  {avg_precision:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix Analysis\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    specificity = tn / (tn + fp)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n=== CLINICAL METRICS ===\")\n",
    "    print(f\"Sensitivity (Recall): {recall:.4f}\")\n",
    "    print(f\"Specificity:          {specificity:.4f}\")\n",
    "    print(f\"PPV (Precision):      {ppv:.4f}\")\n",
    "    print(f\"NPV:                  {npv:.4f}\")\n",
    "    \n",
    "    print(f\"\\n=== CONFUSION MATRIX ===\")\n",
    "    print(f\"True Negatives:  {tn:,}\")\n",
    "    print(f\"False Positives: {fp:,}\")\n",
    "    print(f\"False Negatives: {fn:,}\")\n",
    "    print(f\"True Positives:  {tp:,}\")\n",
    "    \n",
    "    # Clinical interpretation\n",
    "    print(f\"\\n=== CLINICAL INTERPRETATION ===\")\n",
    "    if auc >= 0.9:\n",
    "        interpretation = \"EXCELLENT - Outstanding discrimination\"\n",
    "    elif auc >= 0.8:\n",
    "        interpretation = \"GOOD - Strong predictive performance\"\n",
    "    elif auc >= 0.7:\n",
    "        interpretation = \"FAIR - Acceptable performance\"\n",
    "    elif auc >= 0.6:\n",
    "        interpretation = \"POOR - Limited clinical utility\"\n",
    "    else:\n",
    "        interpretation = \"FAIL - No better than random\"\n",
    "    \n",
    "    print(f\"AUC Interpretation: {interpretation}\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(f\"\\n=== TOP 10 FEATURE IMPORTANCE ===\")\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        for i, row in importance_df.head(10).iterrows():\n",
    "            print(f\"{row['feature']:20}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot curves\n",
    "    plot_roc_curve(y_test, y_pred_proba, \"Optimized Model ROC Curve\")\n",
    "    plot_precision_recall_curve(y_test, y_pred_proba, \"Optimized Model PR Curve\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_roc': auc,\n",
    "        'avg_precision': avg_precision,\n",
    "        'specificity': specificity,\n",
    "        'npv': npv,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models():\n",
    "    \"\"\"Compare original vs optimized model performance\"\"\"\n",
    "    print(\"=== MODEL COMPARISON ===\\n\")\n",
    "    \n",
    "    # Load original metrics\n",
    "    try:\n",
    "        original_metrics = joblib.load('model_metrics.pkl')\n",
    "        print(\"Original Model Performance:\")\n",
    "        for metric, value in original_metrics.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "    except:\n",
    "        print(\"Original model metrics not found\")\n",
    "        original_metrics = {'auc_roc': 0.571}  # From previous results\n",
    "    \n",
    "    # Load optimized metrics\n",
    "    try:\n",
    "        optimized_metrics = joblib.load('optimized_model_metrics.pkl')\n",
    "        print(\"\\nOptimized Model Performance:\")\n",
    "        for metric, value in optimized_metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "    except:\n",
    "        print(\"Optimized model metrics not found\")\n",
    "        return\n",
    "    \n",
    "    # Calculate improvements\n",
    "    print(\"\\n=== PERFORMANCE IMPROVEMENTS ===\")\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']:\n",
    "        if metric in original_metrics and metric in optimized_metrics:\n",
    "            original = original_metrics[metric]\n",
    "            optimized = optimized_metrics[metric]\n",
    "            improvement = ((optimized - original) / original * 100) if original > 0 else 0\n",
    "            print(f\"{metric:15}: {original:.4f} â†’ {optimized:.4f} ({improvement:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED MODEL EVALUATION ===\n",
      "\n",
      "Loading optimized model...\n",
      "Model loaded successfully!\n",
      "Model type: XGBClassifier\n",
      "Number of features: 19\n",
      "\n",
      "Note: Test data needs to be loaded separately\n",
      "Run this after the optimized training completes\n",
      "=== MODEL COMPARISON ===\n",
      "\n",
      "Original Model Performance:\n",
      "  accuracy: 0.7450\n",
      "  precision: 0.7688\n",
      "  recall: 0.9470\n",
      "  f1_score: 0.8487\n",
      "  auc_roc: 0.5714\n",
      "\n",
      "Optimized Model Performance:\n",
      "  accuracy: 0.9692\n",
      "  precision: 0.0822\n",
      "  recall: 0.0693\n",
      "  f1_score: 0.0752\n",
      "  auc_roc: 0.6526\n",
      "  avg_precision: 0.0391\n",
      "\n",
      "=== PERFORMANCE IMPROVEMENTS ===\n",
      "accuracy       : 0.7450 â†’ 0.9692 (+30.1%)\n",
      "precision      : 0.7688 â†’ 0.0822 (-89.3%)\n",
      "recall         : 0.9470 â†’ 0.0693 (-92.7%)\n",
      "f1_score       : 0.8487 â†’ 0.0752 (-91.1%)\n",
      "auc_roc        : 0.5714 â†’ 0.6526 (+14.2%)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=== ADVANCED MODEL EVALUATION ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load optimized model and data\n",
    "        print(\"Loading optimized model...\")\n",
    "        model = joblib.load('optimized_xgboost_sepsis_model.pkl')\n",
    "        scaler = joblib.load('optimized_scaler.pkl')\n",
    "        feature_names = joblib.load('optimized_feature_names.pkl')\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        print(f\"Model type: {type(model).__name__}\")\n",
    "        print(f\"Number of features: {len(feature_names)}\")\n",
    "        \n",
    "        # Load test data (you'll need to recreate this or save it from training)\n",
    "        print(\"\\nNote: Test data needs to be loaded separately\")\n",
    "        print(\"Run this after the optimized training completes\")\n",
    "        \n",
    "        # Compare models\n",
    "        compare_models()\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Model files not found: {e}\")\n",
    "        print(\"Please run Optimized_Model_Training.py first\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
