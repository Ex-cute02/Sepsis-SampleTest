{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3\n", "\"\"\"\n", "Advanced Data Preprocessing and Cleaning Pipeline\n", "Comprehensive preprocessing for optimal sepsis prediction model performance\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n", "from sklearn.experimental import enable_iterative_imputer\n", "from sklearn.impute import KNNImputer, IterativeImputer\n", "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n", "from sklearn.ensemble import IsolationForest\n", "from sklearn.cluster import DBSCAN\n", "from scipy import stats\n", "from scipy.stats import zscore\n", "import joblib\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class AdvancedDataPreprocessor:\n", "    \"\"\"Advanced data preprocessing pipeline for sepsis prediction\"\"\"\n", "    \n", "    def __init__(self, random_state=42):\n", "        self.random_state = random_state\n", "        self.scalers = {}\n", "        self.imputers = {}\n", "        self.transformers = {}\n", "        self.outlier_detectors = {}\n", "        self.feature_stats = {}\n", "        \n", "    def load_and_analyze_data(self, file_path='../Dataset.csv', sample_size=None):\n", "        \"\"\"Load and perform initial data analysis\"\"\"\n", "        print(\"=== LOADING AND ANALYZING DATASET ===\\n\")\n", "        \n", "        # Load data\n", "        df = pd.read_csv(file_path)\n", "        print(f\"Original dataset shape: {df.shape}\")\n", "        print(f\"Total records: {len(df):,}\")\n", "        print(f\"Unique patients: {df['Patient_ID'].nunique():,}\")\n", "        \n", "        # Sample if needed\n", "        if sample_size and len(df) > sample_size:\n", "            print(f\"Sampling {sample_size:,} records...\")\n", "            # Stratified sampling maintaining class distribution\n", "            sepsis_rate = df['SepsisLabel'].mean()\n", "            df_majority = df[df['SepsisLabel'] == 0].sample(\n", "                n=int(sample_size * (1 - sepsis_rate)), \n", "                random_state=self.random_state\n", "            )\n", "            df_minority = df[df['SepsisLabel'] == 1].sample(\n", "                n=int(sample_size * sepsis_rate), \n", "                random_state=self.random_state\n", "            )\n", "            df = pd.concat([df_majority, df_minority]).sample(\n", "                frac=1, random_state=self.random_state\n", "            ).reset_index(drop=True)\n", "            print(f\"Sampled dataset shape: {df.shape}\")\n", "        \n", "        # Basic statistics\n", "        print(f\"\\n=== DATASET OVERVIEW ===\")\n", "        print(f\"Sepsis rate: {df['SepsisLabel'].mean():.3f} ({df['SepsisLabel'].mean()*100:.1f}%)\")\n", "        print(f\"Average age: {df['Age'].mean():.1f} years\")\n", "        print(f\"Gender distribution: {df['Gender'].value_counts().to_dict()}\")\n", "        print(f\"Average ICU stay: {df['ICULOS'].mean():.1f} hours\")\n", "        \n", "        return df\n", "    \n", "    def analyze_missing_patterns(self, df):\n", "        \"\"\"Comprehensive missing data analysis\"\"\"\n", "        print(\"\\n=== MISSING DATA ANALYSIS ===\\n\")\n", "        \n", "        # Missing value statistics\n", "        missing_stats = pd.DataFrame({\n", "            'Missing_Count': df.isnull().sum(),\n", "            'Missing_Percent': (df.isnull().sum() / len(df) * 100).round(2),\n", "            'Data_Type': df.dtypes\n", "        }).sort_values('Missing_Percent', ascending=False)\n", "        \n", "        print(\"Missing value summary:\")\n", "        print(missing_stats.head(15))\n", "        \n", "        # Missing patterns by patient\n", "        patient_missing = df.groupby('Patient_ID').apply(\n", "            lambda x: x.isnull().sum().sum()\n", "        ).describe()\n", "        print(f\"\\nMissing values per patient statistics:\")\n", "        print(patient_missing)\n", "        \n", "        # Correlation between missing values and target\n", "        missing_indicators = df.isnull().astype(int)\n", "        missing_target_corr = missing_indicators.corrwith(df['SepsisLabel']).abs().sort_values(ascending=False)\n", "        print(f\"\\nTop 10 missing patterns correlated with sepsis:\")\n", "        print(missing_target_corr.head(10))\n", "        \n", "        self.feature_stats['missing_stats'] = missing_stats\n", "        return missing_stats\n", "    \n", "    def detect_and_handle_outliers(self, df, features):\n", "        \"\"\"Advanced outlier detection and handling\"\"\"\n", "        print(\"\\n=== OUTLIER DETECTION AND HANDLING ===\\n\")\n", "        \n", "        outlier_summary = {}\n", "        \n", "        for feature in features:\n", "            if feature in df.columns and df[feature].dtype in ['int64', 'float64']:\n", "                # Statistical outlier detection\n", "                Q1 = df[feature].quantile(0.25)\n", "                Q3 = df[feature].quantile(0.75)\n", "                IQR = Q3 - Q1\n", "                lower_bound = Q1 - 1.5 * IQR\n", "                upper_bound = Q3 + 1.5 * IQR\n", "                \n", "                # Z-score outliers\n", "                z_scores = np.abs(zscore(df[feature].dropna()))\n", "                z_outliers = z_scores > 3\n", "                \n", "                # Clinical bounds (domain knowledge)\n", "                clinical_bounds = self.get_clinical_bounds(feature)\n", "                clinical_outliers = (\n", "                    (df[feature] < clinical_bounds['min']) | \n", "                    (df[feature] > clinical_bounds['max'])\n", "                )\n", "                \n", "                outlier_count = clinical_outliers.sum()\n", "                outlier_percent = (outlier_count / len(df)) * 100\n", "                \n", "                outlier_summary[feature] = {\n", "                    'outlier_count': outlier_count,\n", "                    'outlier_percent': outlier_percent,\n", "                    'clinical_bounds': clinical_bounds\n", "                }\n", "                \n", "                # Handle outliers based on clinical knowledge\n", "                if outlier_percent > 0.1:  # If >0.1% outliers\n", "                    print(f\"{feature}: {outlier_count} outliers ({outlier_percent:.2f}%)\")\n", "                    # Cap outliers to clinical bounds\n", "                    df[feature] = df[feature].clip(\n", "                        lower=clinical_bounds['min'], \n", "                        upper=clinical_bounds['max']\n", "                    )\n", "        \n", "        self.feature_stats['outlier_summary'] = outlier_summary\n", "        return df\n", "    \n", "    def get_clinical_bounds(self, feature):\n", "        \"\"\"Get clinically reasonable bounds for features\"\"\"\n", "        clinical_bounds = {\n", "            'HR': {'min': 20, 'max': 250},\n", "            'O2Sat': {'min': 50, 'max': 100},\n", "            'Temp': {'min': 30, 'max': 45},\n", "            'SBP': {'min': 50, 'max': 300},\n", "            'MAP': {'min': 30, 'max': 200},\n", "            'DBP': {'min': 20, 'max': 150},\n", "            'Resp': {'min': 5, 'max': 60},\n", "            'Age': {'min': 0, 'max': 120},\n", "            'ICULOS': {'min': 0, 'max': 2000},\n", "            'Glucose': {'min': 20, 'max': 1000},\n", "            'BUN': {'min': 1, 'max': 200},\n", "            'Creatinine': {'min': 0.1, 'max': 20},\n", "            'WBC': {'min': 0.1, 'max': 100},\n", "            'Hct': {'min': 10, 'max': 70},\n", "            'Hgb': {'min': 3, 'max': 25},\n", "            'Platelets': {'min': 10, 'max': 2000},\n", "            'Lactate': {'min': 0.1, 'max': 30}\n", "        }\n", "        return clinical_bounds.get(feature, {'min': -np.inf, 'max': np.inf})\n", "    \n", "    def advanced_feature_engineering(self, df):\n", "        \"\"\"Comprehensive clinical feature engineering\"\"\"\n", "        print(\"\\n=== ADVANCED FEATURE ENGINEERING ===\\n\")\n", "        \n", "        original_features = df.shape[1]\n", "        \n", "        # 1. Clinical Composite Scores\n", "        print(\"Creating clinical composite scores...\")\n", "        \n", "        # SIRS Criteria (Systemic Inflammatory Response Syndrome)\n", "        sirs_criteria = 0\n", "        if 'Temp' in df.columns:\n", "            sirs_criteria += ((df['Temp'] > 38) | (df['Temp'] < 36)).astype(int)\n", "        if 'HR' in df.columns:\n", "            sirs_criteria += (df['HR'] > 90).astype(int)\n", "        if 'Resp' in df.columns:\n", "            sirs_criteria += (df['Resp'] > 20).astype(int)\n", "        if 'WBC' in df.columns:\n", "            sirs_criteria += ((df['WBC'] > 12) | (df['WBC'] < 4)).astype(int)\n", "        df['SIRS_Score'] = sirs_criteria\n", "        \n", "        # qSOFA Score (quick Sequential Organ Failure Assessment)\n", "        qsofa_score = 0\n", "        if 'SBP' in df.columns:\n", "            qsofa_score += (df['SBP'] <= 100).astype(int)\n", "        if 'Resp' in df.columns:\n", "            qsofa_score += (df['Resp'] >= 22).astype(int)\n", "        # GCS would be ideal but not available, use altered mental status proxy\n", "        df['qSOFA_Score'] = qsofa_score\n", "        \n", "        # 2. Vital Sign Ratios and Indices\n", "        print(\"Creating vital sign ratios...\")\n", "        \n", "        if 'HR' in df.columns and 'SBP' in df.columns:\n", "            df['Shock_Index'] = df['HR'] / (df['SBP'] + 1e-6)\n", "            df['Shock_Index_High'] = (df['Shock_Index'] > 0.9).astype(int)\n", "        \n", "        if 'SBP' in df.columns and 'DBP' in df.columns:\n", "            df['Pulse_Pressure'] = df['SBP'] - df['DBP']\n", "            df['Pulse_Pressure_Narrow'] = (df['Pulse_Pressure'] < 25).astype(int)\n", "        \n", "        if 'MAP' in df.columns:\n", "            df['MAP_Critical'] = (df['MAP'] < 65).astype(int)\n", "        \n", "        # 3. Laboratory Ratios\n", "        print(\"Creating laboratory ratios...\")\n", "        \n", "        if 'BUN' in df.columns and 'Creatinine' in df.columns:\n", "            df['BUN_Creatinine_Ratio'] = df['BUN'] / (df['Creatinine'] + 1e-6)\n", "            df['AKI_Risk'] = (df['BUN_Creatinine_Ratio'] > 20).astype(int)\n", "        \n", "        if 'WBC' in df.columns and 'Hct' in df.columns:\n", "            df['WBC_Hct_Ratio'] = df['WBC'] / (df['Hct'] + 1e-6)\n", "        \n", "        # 4. Time-based Features\n", "        print(\"Creating temporal features...\")\n", "        \n", "        if 'ICULOS' in df.columns:\n", "            df['ICU_Day'] = (df['ICULOS'] / 24).astype(int)\n", "            df['ICU_Hour_of_Day'] = df['ICULOS'] % 24\n", "            df['ICU_Early'] = (df['ICULOS'] <= 6).astype(int)\n", "            df['ICU_Late'] = (df['ICULOS'] > 72).astype(int)\n", "        \n", "        # 5. Age-based Risk Categories\n", "        print(\"Creating age-based features...\")\n", "        \n", "        if 'Age' in df.columns:\n", "            df['Age_Pediatric'] = (df['Age'] < 18).astype(int)\n", "            df['Age_Adult'] = ((df['Age'] >= 18) & (df['Age'] < 65)).astype(int)\n", "            df['Age_Elderly'] = ((df['Age'] >= 65) & (df['Age'] < 80)).astype(int)\n", "            df['Age_Very_Elderly'] = (df['Age'] >= 80).astype(int)\n", "            df['Age_Squared'] = df['Age'] ** 2\n", "        \n", "        # 6. Clinical Severity Indicators\n", "        print(\"Creating severity indicators...\")\n", "        \n", "        # Multi-organ dysfunction indicators\n", "        organ_dysfunction = 0\n", "        if 'SBP' in df.columns:\n", "            organ_dysfunction += (df['SBP'] < 90).astype(int)  # Cardiovascular\n", "        if 'O2Sat' in df.columns:\n", "            organ_dysfunction += (df['O2Sat'] < 90).astype(int)  # Respiratory\n", "        if 'Creatinine' in df.columns:\n", "            organ_dysfunction += (df['Creatinine'] > 2.0).astype(int)  # Renal\n", "        if 'Platelets' in df.columns:\n", "            organ_dysfunction += (df['Platelets'] < 100).astype(int)  # Hematologic\n", "        df['Organ_Dysfunction_Count'] = organ_dysfunction\n", "        \n", "        # 7. Interaction Features\n", "        print(\"Creating interaction features...\")\n", "        \n", "        if 'Age' in df.columns and 'SIRS_Score' in df.columns:\n", "            df['Age_SIRS_Interaction'] = df['Age'] * df['SIRS_Score']\n", "        \n", "        if 'Gender' in df.columns and 'Age' in df.columns:\n", "            df['Gender_Age_Interaction'] = df['Gender'] * df['Age']\n", "        \n", "        new_features = df.shape[1] - original_features\n", "        print(f\"Created {new_features} new features\")\n", "        print(f\"Total features: {df.shape[1]}\")\n", "        \n", "        return df\n", "    \n", "    def advanced_imputation(self, df, target_col='SepsisLabel'):\n", "        \"\"\"Advanced missing value imputation strategies\"\"\"\n", "        print(\"\\n=== ADVANCED MISSING VALUE IMPUTATION ===\\n\")\n", "        \n", "        # Separate features by missing percentage\n", "        missing_stats = df.isnull().mean()\n", "        \n", "        low_missing = missing_stats[missing_stats <= 0.3].index.tolist()\n", "        medium_missing = missing_stats[(missing_stats > 0.3) & (missing_stats <= 0.7)].index.tolist()\n", "        high_missing = missing_stats[missing_stats > 0.7].index.tolist()\n", "        \n", "        # Remove non-feature columns\n", "        exclude_cols = ['Patient_ID', 'Hour', 'Unit1', 'Unit2', 'HospAdmTime', 'Unnamed: 0', target_col]\n", "        low_missing = [col for col in low_missing if col not in exclude_cols]\n", "        medium_missing = [col for col in medium_missing if col not in exclude_cols]\n", "        high_missing = [col for col in high_missing if col not in exclude_cols]\n", "        \n", "        print(f\"Low missing (<30%): {len(low_missing)} features\")\n", "        print(f\"Medium missing (30-70%): {len(medium_missing)} features\")\n", "        print(f\"High missing (>70%): {len(high_missing)} features\")\n", "        \n", "        # Strategy 1: KNN Imputation for low missing features\n", "        if low_missing:\n", "            print(\"Applying KNN imputation for low missing features...\")\n", "            knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n", "            df[low_missing] = knn_imputer.fit_transform(df[low_missing])\n", "            self.imputers['knn'] = knn_imputer\n", "        \n", "        # Strategy 2: Iterative imputation for medium missing features\n", "        if medium_missing:\n", "            print(\"Applying iterative imputation for medium missing features...\")\n", "            iterative_imputer = IterativeImputer(\n", "                estimator=None,  # Uses BayesianRidge by default\n", "                max_iter=10,\n", "                random_state=self.random_state\n", "            )\n", "            df[medium_missing] = iterative_imputer.fit_transform(df[medium_missing])\n", "            self.imputers['iterative'] = iterative_imputer\n", "        \n", "        # Strategy 3: Clinical-informed imputation for high missing features\n", "        if high_missing:\n", "            print(\"Applying clinical-informed imputation for high missing features...\")\n", "            for col in high_missing:\n", "                if col in df.columns:\n", "                    # Create missing indicator\n", "                    df[f'{col}_Missing'] = df[col].isnull().astype(int)\n", "                    \n", "                    # Impute with clinical normal values\n", "                    clinical_normals = {\n", "                        'Lactate': 1.5, 'AST': 25, 'BUN': 15, 'Alkalinephos': 100,\n", "                        'Calcium': 9.5, 'Chloride': 100, 'Creatinine': 1.0,\n", "                        'Bilirubin_direct': 0.2, 'Glucose': 100, 'Magnesium': 2.0,\n", "                        'Phosphate': 3.5, 'Potassium': 4.0, 'Bilirubin_total': 1.0,\n", "                        'TroponinI': 0.01, 'PTT': 30, 'Fibrinogen': 300,\n", "                        'BaseExcess': 0, 'HCO3': 24, 'PaCO2': 40, 'pH': 7.4,\n", "                        'SaO2': 98, 'EtCO2': 35, 'FiO2': 21\n", "                    }\n", "                    \n", "                    normal_value = clinical_normals.get(col, df[col].median())\n", "                    df[col].fillna(normal_value, inplace=True)\n", "        \n", "        print(\"Imputation completed\")\n", "        print(f\"Remaining missing values: {df.isnull().sum().sum()}\")\n", "        \n", "        return df\n", "    \n", "    def feature_transformation(self, df, target_col='SepsisLabel'):\n", "        \"\"\"Advanced feature transformations\"\"\"\n", "        print(\"\\n=== FEATURE TRANSFORMATIONS ===\\n\")\n", "        \n", "        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n", "        numeric_features = [col for col in numeric_features if col != target_col]\n", "        \n", "        # 1. Skewness correction\n", "        print(\"Analyzing and correcting skewness...\")\n", "        skewed_features = []\n", "        \n", "        for feature in numeric_features:\n", "            if feature in df.columns:\n", "                skewness = df[feature].skew()\n", "                if abs(skewness) > 1:  # Highly skewed\n", "                    skewed_features.append(feature)\n", "                    print(f\"{feature}: skewness = {skewness:.2f}\")\n", "        \n", "        # Apply power transformation to highly skewed features\n", "        if skewed_features:\n", "            print(f\"Applying power transformation to {len(skewed_features)} skewed features...\")\n", "            power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n", "            df[skewed_features] = power_transformer.fit_transform(df[skewed_features])\n", "            self.transformers['power'] = power_transformer\n", "        \n", "        # 2. Normalization/Standardization\n", "        print(\"Applying robust scaling...\")\n", "        scaler = RobustScaler()\n", "        df[numeric_features] = scaler.fit_transform(df[numeric_features])\n", "        self.scalers['robust'] = scaler\n", "        \n", "        return df\n", "    \n", "    def intelligent_feature_selection(self, df, target_col='SepsisLabel'):\n", "        \"\"\"Intelligent feature selection based on clinical relevance and statistical significance\"\"\"\n", "        print(\"\\n=== INTELLIGENT FEATURE SELECTION ===\\n\")\n", "        \n", "        X = df.drop(columns=[target_col, 'Patient_ID'], errors='ignore')\n", "        y = df[target_col]\n", "        \n", "        original_features = X.shape[1]\n", "        print(f\"Starting with {original_features} features\")\n", "        \n", "        # 1. Remove constant and quasi-constant features\n", "        print(\"Removing constant and quasi-constant features...\")\n", "        variance_selector = VarianceThreshold(threshold=0.01)\n", "        X_variance = variance_selector.fit_transform(X)\n", "        selected_features = X.columns[variance_selector.get_support()].tolist()\n", "        X = X[selected_features]\n", "        print(f\"After variance threshold: {X.shape[1]} features\")\n", "        \n", "        # 2. Statistical significance test\n", "        print(\"Selecting statistically significant features...\")\n", "        # Use all features if reasonable number, otherwise select top 50\n", "        k = min(50, X.shape[1])\n", "        selector = SelectKBest(score_func=f_classif, k=k)\n", "        X_selected = selector.fit_transform(X, y)\n", "        selected_feature_names = X.columns[selector.get_support()].tolist()\n", "        \n", "        # Get feature scores\n", "        feature_scores = pd.DataFrame({\n", "            'feature': X.columns,\n", "            'score': selector.scores_,\n", "            'p_value': selector.pvalues_\n", "        }).sort_values('score', ascending=False)\n", "        \n", "        print(f\"After statistical selection: {len(selected_feature_names)} features\")\n", "        print(\"\\nTop 15 most significant features:\")\n", "        print(feature_scores.head(15))\n", "        \n", "        # 3. Clinical relevance filtering\n", "        print(\"Applying clinical relevance filtering...\")\n", "        clinical_priority_features = [\n", "            'SIRS_Score', 'qSOFA_Score', 'Shock_Index', 'MAP_Critical',\n", "            'Organ_Dysfunction_Count', 'Age_Elderly', 'ICU_Late',\n", "            'HR', 'Temp', 'SBP', 'Resp', 'Age', 'ICULOS'\n", "        ]\n", "        \n", "        # Ensure clinical priority features are included if available\n", "        final_features = selected_feature_names.copy()\n", "        for feature in clinical_priority_features:\n", "            if feature in X.columns and feature not in final_features:\n", "                final_features.append(feature)\n", "        \n", "        print(f\"Final feature count: {len(final_features)}\")\n", "        \n", "        # Create final dataset\n", "        final_df = df[final_features + [target_col, 'Patient_ID']].copy()\n", "        \n", "        self.feature_stats['selected_features'] = final_features\n", "        self.feature_stats['feature_scores'] = feature_scores\n", "        \n", "        return final_df\n", "    \n", "    def generate_preprocessing_report(self, original_df, processed_df):\n", "        \"\"\"Generate comprehensive preprocessing report\"\"\"\n", "        print(\"\\n\" + \"=\"*60)\n", "        print(\"COMPREHENSIVE PREPROCESSING REPORT\")\n", "        print(\"=\"*60)\n", "        \n", "        print(f\"\\n\u00f0\u0178\u201c\u0160 DATASET TRANSFORMATION SUMMARY\")\n", "        print(f\"Original shape: {original_df.shape}\")\n", "        print(f\"Processed shape: {processed_df.shape}\")\n", "        print(f\"Features added: {processed_df.shape[1] - original_df.shape[1]}\")\n", "        \n", "        print(f\"\\n\u00f0\u0178\u201d\u00a7 PREPROCESSING STEPS APPLIED\")\n", "        print(\"\u00e2\u0153\u2026 Advanced outlier detection and clinical bounds enforcement\")\n", "        print(\"\u00e2\u0153\u2026 Comprehensive feature engineering (clinical scores, ratios, interactions)\")\n", "        print(\"\u00e2\u0153\u2026 Multi-strategy missing value imputation\")\n", "        print(\"\u00e2\u0153\u2026 Skewness correction with power transformations\")\n", "        print(\"\u00e2\u0153\u2026 Robust scaling for outlier resistance\")\n", "        print(\"\u00e2\u0153\u2026 Intelligent feature selection with clinical prioritization\")\n", "        \n", "        print(f\"\\n\u00f0\u0178\u201c\u02c6 DATA QUALITY IMPROVEMENTS\")\n", "        original_missing = original_df.isnull().sum().sum()\n", "        processed_missing = processed_df.isnull().sum().sum()\n", "        print(f\"Missing values: {original_missing:,} \u00e2\u2020\u2019 {processed_missing:,}\")\n", "        print(f\"Missing value reduction: {((original_missing - processed_missing) / original_missing * 100):.1f}%\")\n", "        \n", "        if 'selected_features' in self.feature_stats:\n", "            print(f\"\\n\u00f0\u0178\u017d\u00af FEATURE SELECTION RESULTS\")\n", "            print(f\"Selected features: {len(self.feature_stats['selected_features'])}\")\n", "            print(\"Top clinical features included:\")\n", "            clinical_features = [f for f in self.feature_stats['selected_features'] \n", "                               if any(keyword in f.lower() for keyword in \n", "                                    ['sirs', 'qsofa', 'shock', 'organ', 'age', 'icu'])]\n", "            for feature in clinical_features[:10]:\n", "                print(f\"  \u00e2\u20ac\u00a2 {feature}\")\n", "        \n", "        return processed_df\n", "    \n", "    def save_preprocessing_artifacts(self, processed_df, prefix='advanced'):\n", "        \"\"\"Save all preprocessing artifacts\"\"\"\n", "        print(f\"\\n\u00f0\u0178\u2019\u00be SAVING PREPROCESSING ARTIFACTS\")\n", "        \n", "        # Save processed dataset\n", "        processed_df.to_csv(f'{prefix}_processed_dataset.csv', index=False)\n", "        print(f\"\u00e2\u0153\u2026 Saved: {prefix}_processed_dataset.csv\")\n", "        \n", "        # Save preprocessing components\n", "        joblib.dump(self.scalers, f'{prefix}_scalers.pkl')\n", "        joblib.dump(self.imputers, f'{prefix}_imputers.pkl')\n", "        joblib.dump(self.transformers, f'{prefix}_transformers.pkl')\n", "        joblib.dump(self.feature_stats, f'{prefix}_feature_stats.pkl')\n", "        \n", "        print(f\"\u00e2\u0153\u2026 Saved: {prefix}_scalers.pkl\")\n", "        print(f\"\u00e2\u0153\u2026 Saved: {prefix}_imputers.pkl\")\n", "        print(f\"\u00e2\u0153\u2026 Saved: {prefix}_transformers.pkl\")\n", "        print(f\"\u00e2\u0153\u2026 Saved: {prefix}_feature_stats.pkl\")\n", "        \n", "        return True"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n", "    \"\"\"Main preprocessing pipeline\"\"\"\n", "    print(\"=== ADVANCED DATA PREPROCESSING PIPELINE ===\\n\")\n", "    \n", "    # Initialize preprocessor\n", "    preprocessor = AdvancedDataPreprocessor(random_state=42)\n", "    \n", "    # Load and analyze data\n", "    df = preprocessor.load_and_analyze_data(sample_size=300000)  # Larger sample\n", "    original_df = df.copy()\n", "    \n", "    # Comprehensive preprocessing pipeline\n", "    missing_stats = preprocessor.analyze_missing_patterns(df)\n", "    \n", "    # Select features with reasonable availability\n", "    available_features = missing_stats[missing_stats['Missing_Percent'] < 80].index.tolist()\n", "    available_features = [f for f in available_features if f not in \n", "                         ['Unnamed: 0', 'Hour', 'Unit1', 'Unit2', 'HospAdmTime', 'Patient_ID', 'SepsisLabel']]\n", "    \n", "    df = preprocessor.detect_and_handle_outliers(df, available_features)\n", "    df = preprocessor.advanced_feature_engineering(df)\n", "    df = preprocessor.advanced_imputation(df)\n", "    df = preprocessor.feature_transformation(df)\n", "    df = preprocessor.intelligent_feature_selection(df)\n", "    \n", "    # Generate report and save artifacts\n", "    processed_df = preprocessor.generate_preprocessing_report(original_df, df)\n", "    preprocessor.save_preprocessing_artifacts(processed_df)\n", "    \n", "    print(f\"\\n\u00f0\u0178\u017d\u2030 PREPROCESSING COMPLETE!\")\n", "    print(\"Ready for advanced model training with cleaned and optimized dataset.\")\n", "    \n", "    return processed_df, preprocessor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    processed_df, preprocessor = main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}